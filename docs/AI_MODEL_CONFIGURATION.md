# AIæ¨¡å‹é…ç½®ç³»ç»Ÿè®¾è®¡

## æ¦‚è¿°
ç³»ç»Ÿæ”¯æŒå¤šç§AIå¤§æ¨¡å‹æ¥å…¥ï¼Œç”±Masterç®¡ç†å‘˜ç»Ÿä¸€é…ç½®å’Œç®¡ç†ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… æ”¯æŒå¤šç§AIæ¨¡å‹ï¼ˆé€šä¹‰åƒé—®ã€MiniMaxã€OpenAIã€Claudeç­‰ï¼‰
- âœ… Masteråå°å¯é…ç½®å’Œåˆ‡æ¢æ¨¡å‹
- âœ… **æ¨¡å‹çƒ­åˆ‡æ¢**ï¼šåˆ‡æ¢æ¨¡å‹ä¸å½±å“å¯¹è¯å†å²å’Œè®°å¿†
- âœ… å¯¹è¯å†å²ä¸æ¨¡å‹è§£è€¦ï¼Œæ”¯æŒè·¨æ¨¡å‹å»¶ç»­å¯¹è¯

## æ”¯æŒçš„AIæ¨¡å‹

### ä¼˜å…ˆæ”¯æŒï¼ˆå›½äº§æ¨¡å‹ï¼‰

#### 1. é€šä¹‰åƒé—® (é˜¿é‡Œäº‘) - æ¨è â­
**ä¼˜åŠ¿**ï¼š
- ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œç‰¹åˆ«é€‚åˆåŒ»ç–—é¢†åŸŸ
- ä¸é˜¿é‡Œäº‘ç”Ÿæ€é›†æˆè‰¯å¥½ï¼ˆOSSã€è¯­éŸ³è¯†åˆ«ç­‰ï¼‰
- æ€§ä»·æ¯”é«˜
- å›½å†…è®¿é—®ç¨³å®šï¼Œä½å»¶è¿Ÿ

**æ¨¡å‹é€‰æ‹©**ï¼š
- `qwen-max`: æœ€å¼ºèƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ç—…å†åˆ†æ
- `qwen-plus`: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
- `qwen-turbo`: å¿«é€Ÿå“åº”ï¼Œé€‚åˆç®€å•å¯¹è¯

**APIæ–‡æ¡£**: https://help.aliyun.com/zh/dashscope/

#### 2. MiniMax - æ¨è â­
**ä¼˜åŠ¿**ï¼š
- å›½äº§å¤§æ¨¡å‹ï¼Œä¸­æ–‡èƒ½åŠ›ä¼˜ç§€
- æ”¯æŒé•¿ä¸Šä¸‹æ–‡
- ä»·æ ¼åˆç†
- APIç¨³å®š

**æ¨¡å‹é€‰æ‹©**ï¼š
- `abab6-chat`: ç»¼åˆèƒ½åŠ›å¼º
- `abab5.5-chat`: æ€§ä»·æ¯”ç‰ˆæœ¬

**APIæ–‡æ¡£**: https://api.minimax.chat/

### å›½é™…æ¨¡å‹ï¼ˆå®Œæ•´æ”¯æŒï¼‰

#### 3. OpenAI GPT â­
**ä¼˜åŠ¿**ï¼š
- ç»¼åˆèƒ½åŠ›æœ€å¼ºï¼Œå¤šæ¨¡æ€æ”¯æŒ
- å…¨çƒæœ€æˆç†Ÿçš„APIç”Ÿæ€
- Function Callingæ”¯æŒå®Œå–„
- æ–‡æ¡£å’Œç¤¾åŒºèµ„æºä¸°å¯Œ

**æ¨¡å‹é€‰æ‹©**ï¼š
- `gpt-4o`: æœ€æ–°æ——èˆ°ï¼Œå¤šæ¨¡æ€ï¼Œé€Ÿåº¦å¿«
- `gpt-4-turbo`: 128Kä¸Šä¸‹æ–‡ï¼Œæ€§èƒ½å¼ºå¤§
- `gpt-4`: ç»å…¸ç‰ˆæœ¬ï¼Œç¨³å®šå¯é 
- `gpt-3.5-turbo`: æˆæœ¬ä¼˜åŒ–ï¼Œé€‚åˆç®€å•ä»»åŠ¡

**APIé…ç½®**ï¼š
```python
OPENAI_CONFIG = {
    "api_key": "sk-xxxxxxxxxx",
    "base_url": "https://api.openai.com/v1",  # å¯æ›¿æ¢ä¸ºä»£ç†
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 4096,
    "timeout": 30
}
```

**APIæ–‡æ¡£**: https://platform.openai.com/docs/

#### 4. Anthropic Claude â­
**ä¼˜åŠ¿**ï¼š
- é€»è¾‘æ¨ç†èƒ½åŠ›å¼º
- é•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆ200K tokensï¼‰
- å®‰å…¨æ€§å’Œå¯æ§æ€§å¥½
- ä¸­æ–‡ç†è§£èƒ½åŠ›ä¼˜ç§€

**æ¨¡å‹é€‰æ‹©**ï¼š
- `claude-3-opus`: æœ€å¼ºèƒ½åŠ›ï¼Œå¤æ‚ä»»åŠ¡é¦–é€‰
- `claude-3-sonnet`: å¹³è¡¡é€‰æ‹©ï¼Œæ—¥å¸¸ä½¿ç”¨
- `claude-3-haiku`: å¿«é€Ÿå“åº”ï¼Œç®€å•ä»»åŠ¡

**APIé…ç½®**ï¼š
```python
CLAUDE_CONFIG = {
    "api_key": "sk-ant-xxxxxxxxxx",
    "base_url": "https://api.anthropic.com",
    "model": "claude-3-sonnet-20240229",
    "temperature": 0.7,
    "max_tokens": 4096,
    "timeout": 30
}
```

**APIæ–‡æ¡£**: https://docs.anthropic.com/

#### 5. Kimi (æœˆä¹‹æš—é¢ Moonshot) â­
**ä¼˜åŠ¿**ï¼š
- è¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆ200K tokensï¼‰
- ä¸­æ–‡ç†è§£èƒ½åŠ›ä¼˜ç§€
- å›½äº§æ¨¡å‹ï¼Œè®¿é—®ç¨³å®š
- APIå…¼å®¹OpenAIæ ¼å¼

**æ¨¡å‹é€‰æ‹©**ï¼š
- `moonshot-v1-8k`: 8Kä¸Šä¸‹æ–‡ï¼Œé€Ÿåº¦å¿«
- `moonshot-v1-32k`: 32Kä¸Šä¸‹æ–‡ï¼Œå¹³è¡¡
- `moonshot-v1-128k`: 128Kä¸Šä¸‹æ–‡ï¼Œé•¿æ–‡æ¡£

**APIé…ç½®**ï¼š
```python
KIMI_CONFIG = {
    "api_key": "sk-xxxxxxxxxx",
    "base_url": "https://api.moonshot.cn/v1",
    "model": "moonshot-v1-32k",
    "temperature": 0.7,
    "max_tokens": 4096
}
```

**APIæ–‡æ¡£**: https://platform.moonshot.cn/docs/

#### 6. è…¾è®¯å…ƒå®/æ··å…ƒ â­
**ä¼˜åŠ¿**ï¼š
- è…¾è®¯ç”Ÿæ€é›†æˆï¼ˆå¾®ä¿¡å°ç¨‹åºå‹å¥½ï¼‰
- ä¸­æ–‡èƒ½åŠ›å¼º
- æ”¯æŒå›¾åƒç†è§£
- ä¼ä¸šçº§æœåŠ¡ç¨³å®š

**æ¨¡å‹é€‰æ‹©**ï¼š
- `hunyuan-lite`: è½»é‡ç‰ˆï¼Œé€Ÿåº¦å¿«
- `hunyuan-standard`: æ ‡å‡†ç‰ˆï¼Œå¹³è¡¡
- `hunyuan-pro`: ä¸“ä¸šç‰ˆï¼Œèƒ½åŠ›æœ€å¼º

**APIé…ç½®**ï¼š
```python
HUNYUAN_CONFIG = {
    "secret_id": "AKIDxxxxxxxxxx",
    "secret_key": "xxxxxxxxxx",
    "region": "ap-guangzhou",
    "model": "hunyuan-standard",
    "temperature": 0.7,
    "max_tokens": 4096
}
```

**APIæ–‡æ¡£**: https://cloud.tencent.com/document/product/1729

#### 7. Google Gemini
**ä¼˜åŠ¿**ï¼š
- å¤šæ¨¡æ€èƒ½åŠ›å¼ºï¼ˆå›¾åƒã€è§†é¢‘ç†è§£ï¼‰
- æ¨ç†èƒ½åŠ›ä¼˜ç§€
- æ”¯æŒé•¿ä¸Šä¸‹æ–‡

**æ¨¡å‹é€‰æ‹©**ï¼š
- `gemini-1.5-pro`: æœ€å¼ºèƒ½åŠ›ï¼Œ100ä¸‡tokensä¸Šä¸‹æ–‡
- `gemini-1.5-flash`: å¿«é€Ÿå“åº”
- `gemini-1.0-pro`: ç»å…¸ç‰ˆæœ¬

**APIé…ç½®**ï¼š
```python
GEMINI_CONFIG = {
    "api_key": "AIzaxxxxxxxxxx",
    "model": "gemini-1.5-pro",
    "temperature": 0.7,
    "max_tokens": 4096
}
```

**APIæ–‡æ¡£**: https://ai.google.dev/docs

#### 8. DeepSeek
**ä¼˜åŠ¿**ï¼š
- å›½äº§å¼€æºæ¨¡å‹ï¼Œæˆæœ¬æä½
- æ”¯æŒè‡ªéƒ¨ç½²
- APIå…¼å®¹OpenAIæ ¼å¼

**æ¨¡å‹é€‰æ‹©**ï¼š
- `deepseek-chat`: å¯¹è¯æ¨¡å‹
- `deepseek-coder`: ä»£ç ä¸“ç”¨

**APIé…ç½®**ï¼š
```python
DEEPSEEK_CONFIG = {
    "api_key": "sk-xxxxxxxxxx",
    "base_url": "https://api.deepseek.com/v1",
    "model": "deepseek-chat",
    "temperature": 0.7,
    "max_tokens": 4096
}
```

## æ¨¡å‹èƒ½åŠ›å¯¹æ¯”

| æ¨¡å‹ | ä¸­æ–‡èƒ½åŠ› | æ¨ç†èƒ½åŠ› | ä¸Šä¸‹æ–‡ | æˆæœ¬ | å»¶è¿Ÿ | æ¨èåœºæ™¯ |
|------|---------|---------|--------|------|------|---------|
| é€šä¹‰åƒé—®-Max | â­â­â­â­â­ | â­â­â­â­ | 32K | ä½ | ä½ | æ—¥å¸¸ä½¿ç”¨ |
| MiniMax | â­â­â­â­ | â­â­â­â­ | 200K | ä½ | ä½ | é•¿å¯¹è¯ |
| **Kimi** | â­â­â­â­â­ | â­â­â­â­ | **200K** | ä½ | ä½ | è¶…é•¿ä¸Šä¸‹æ–‡ |
| **è…¾è®¯æ··å…ƒ** | â­â­â­â­â­ | â­â­â­â­ | 32K | ä½ | ä½ | å¾®ä¿¡ç”Ÿæ€ |
| GPT-4o | â­â­â­â­ | â­â­â­â­â­ | 128K | é«˜ | ä¸­ | å¤æ‚åˆ†æ |
| Claude-3-Sonnet | â­â­â­â­ | â­â­â­â­â­ | 200K | ä¸­ | ä¸­ | é€»è¾‘æ¨ç† |
| **Gemini-1.5-Pro** | â­â­â­ | â­â­â­â­â­ | **1M** | ä¸­ | ä¸­ | å¤šæ¨¡æ€ |
| DeepSeek | â­â­â­â­ | â­â­â­ | 32K | æä½ | ä½ | æˆæœ¬æ•æ„Ÿ |

### å›½äº§æ¨¡å‹æ¨èæ’åºï¼ˆç»¼åˆè€ƒè™‘ä¸­æ–‡èƒ½åŠ›ã€ç¨³å®šæ€§ã€æˆæœ¬ï¼‰
1. **é€šä¹‰åƒé—®** - é˜¿é‡Œäº‘ç”Ÿæ€ï¼Œæœ€ç¨³å®š
2. **Kimi** - è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œé€‚åˆå¤æ‚ç—…å†
3. **è…¾è®¯æ··å…ƒ** - å¾®ä¿¡å°ç¨‹åºåŸç”Ÿæ”¯æŒ
4. **MiniMax** - æ€§ä»·æ¯”é«˜
5. **DeepSeek** - æˆæœ¬æœ€ä½

## æ¨¡å‹çƒ­åˆ‡æ¢ä¸å¯¹è¯ä¿ç•™ ğŸ”¥

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

**å¯¹è¯å†å²ä¸æ¨¡å‹å®Œå…¨è§£è€¦**ï¼š
- å¯¹è¯å†å²å­˜å‚¨åœ¨ `conversations` å’Œ `conversation_messages` è¡¨
- æ¨¡å‹é…ç½®å­˜å‚¨åœ¨ `ai_models` è¡¨
- åˆ‡æ¢æ¨¡å‹åªæ”¹å˜"ä½¿ç”¨å“ªä¸ªæ¨¡å‹å›å¤"ï¼Œä¸å½±å“å·²æœ‰å¯¹è¯

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¯¹è¯å†å²ï¼ˆæŒä¹…åŒ–ï¼‰                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ æ¶ˆæ¯1: ç”¨æˆ· - "é˜³å…‰å…»æ®–åœºçš„é¸¡å’³å—½..."            â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯2: AI(é€šä¹‰) - "å¥½çš„ï¼Œæˆ‘è®°å½•äº†..."           â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯3: ç”¨æˆ· - "å¤§æ¦‚200åª"                       â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯4: AI(é€šä¹‰) - "è¯·é—®æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ"            â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ­¤æ—¶Masteråˆ‡æ¢åˆ°GPT-4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯5: ç”¨æˆ· - "ç™½ç¾½è‚‰é¸¡"                        â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯6: AI(GPT-4) - "æ˜ç™½äº†ï¼Œ35æ—¥é¾„çš„ç™½ç¾½..."    â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ­¤æ—¶Masteråˆ‡æ¢åˆ°Claude â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯7: ç”¨æˆ· - "è¿˜æœ‰å‘¼å¸å›°éš¾"                    â”‚   â”‚
â”‚  â”‚ æ¶ˆæ¯8: AI(Claude) - "ç»¼åˆç—‡çŠ¶æ¥çœ‹..."           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â”‚  âœ… æ‰€æœ‰æ¶ˆæ¯éƒ½ä¿ç•™                                      â”‚
â”‚  âœ… æ–°æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡                               â”‚
â”‚  âœ… æå–çš„ä¿¡æ¯æŒç»­ç´¯ç§¯                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯å°ç¨‹åº  â”‚â”€â”€â”€â”€â–¶â”‚   åç«¯API    â”‚â”€â”€â”€â”€â–¶â”‚  AIé€‚é…å™¨å±‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
       å¯¹è¯å†å²                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â–¼        â–¼        â–¼
    â”‚ messages â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è¡¨      â”‚                   â”‚ é€šä¹‰   â”‚â”‚ GPT-4  â”‚â”‚ Claude â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ åƒé—®   â”‚â”‚        â”‚â”‚        â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    æå–çš„ä¿¡æ¯                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
    â”‚ context  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  JSONB   â”‚   æ¯æ¬¡å›å¤éƒ½æ›´æ–°ç´¯ç§¯ä¿¡æ¯
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®å®ç°

#### 1. å¯¹è¯æ¶ˆæ¯å­˜å‚¨ï¼ˆæ¨¡å‹æ— å…³ï¼‰
```sql
-- conversation_messages è¡¨
CREATE TABLE conversation_messages (
    id UUID PRIMARY KEY,
    conversation_id UUID REFERENCES conversations(id),
    role VARCHAR(20) NOT NULL,        -- user / assistant
    content TEXT NOT NULL,
    audio_url TEXT,                   -- è¯­éŸ³æ–‡ä»¶URL
    model_used VARCHAR(100),          -- è®°å½•ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä»…ä¾›å‚è€ƒï¼‰
    extracted_info JSONB,             -- AIæå–çš„ä¿¡æ¯
    confidence_scores JSONB,          -- ç½®ä¿¡åº¦
    timestamp TIMESTAMP NOT NULL
);
```

#### 2. å¯¹è¯ä¸Šä¸‹æ–‡ç´¯ç§¯ï¼ˆè·¨æ¨¡å‹å…±äº«ï¼‰
```sql
-- conversations è¡¨çš„ context å­—æ®µ
{
  "collected_info": {
    "farm_name": "é˜³å…‰å…»æ®–åœº",
    "onset_date": "2026-01-15",
    "poultry_type": "é¸¡",
    "breed": "ç™½ç¾½è‚‰é¸¡",
    "affected_count": 200,
    "symptoms": ["å’³å—½", "å‘¼å¸å›°éš¾"]
  },
  "pending_questions": ["æ—¥é¾„", "æ€»ç¾¤æ•°é‡"],
  "confidence": {
    "farm_name": 0.98,
    "onset_date": 0.95,
    "symptoms": 0.99
  }
}
```

#### 3. æ¨¡å‹åˆ‡æ¢ä¸å½±å“å¯¹è¯
```python
class ConversationService:
    async def send_message(
        self,
        conversation_id: UUID,
        user_message: str,
        audio_url: str = None
    ) -> dict:
        """å‘é€æ¶ˆæ¯å¹¶è·å–AIå›å¤"""

        # 1. è·å–å¯¹è¯å†å²ï¼ˆæ‰€æœ‰å†å²æ¶ˆæ¯ï¼‰
        conversation = await self.get_conversation(conversation_id)
        message_history = await self.get_messages(conversation_id)

        # 2. è·å–å½“å‰é»˜è®¤æ¨¡å‹ï¼ˆå¯èƒ½å·²è¢«Masteråˆ‡æ¢ï¼‰
        current_model = await self.get_default_model()

        # 3. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ‰€æœ‰å†å²ï¼Œä¸ç®¡ä¹‹å‰ç”¨çš„ä»€ä¹ˆæ¨¡å‹ï¼‰
        messages = self._build_context(message_history, conversation.context)

        # 4. æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})

        # 5. è°ƒç”¨å½“å‰æ¨¡å‹
        adapter = LLMAdapterFactory.create_adapter(current_model)
        response = await adapter.chat_completion(messages)

        # 6. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        await self.save_message(
            conversation_id=conversation_id,
            role="user",
            content=user_message,
            audio_url=audio_url,
            model_used=None  # ç”¨æˆ·æ¶ˆæ¯ä¸è®°å½•æ¨¡å‹
        )

        # 7. ä¿å­˜AIå›å¤ï¼ˆè®°å½•ä½¿ç”¨çš„æ¨¡å‹ï¼‰
        await self.save_message(
            conversation_id=conversation_id,
            role="assistant",
            content=response["content"],
            model_used=f"{current_model.provider}/{current_model.model_name}",
            extracted_info=response.get("extracted_info"),
            confidence_scores=response.get("confidence_scores")
        )

        # 8. æ›´æ–°ç´¯ç§¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        await self.update_context(conversation_id, response.get("extracted_info"))

        return response

    def _build_context(
        self,
        message_history: list,
        accumulated_context: dict
    ) -> list:
        """æ„å»ºå‘é€ç»™AIçš„ä¸Šä¸‹æ–‡"""

        messages = [
            {
                "role": "system",
                "content": self._build_system_prompt(accumulated_context)
            }
        ]

        # æ·»åŠ æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆä¸ç®¡ä¹‹å‰ç”¨çš„ä»€ä¹ˆæ¨¡å‹ï¼‰
        for msg in message_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })

        return messages

    def _build_system_prompt(self, context: dict) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ŒåŒ…å«å·²æ”¶é›†çš„ä¿¡æ¯"""

        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…½åŒ»åŠ©æ‰‹ï¼Œè´Ÿè´£å¸®åŠ©å…½åŒ»åˆ›å»ºç¦½ç—…ç”µå­ç—…å†ã€‚
è¯·ç»§ç»­ä¹‹å‰çš„å¯¹è¯ï¼Œå¸®åŠ©å…½åŒ»å®Œå–„ç—…å†ä¿¡æ¯ã€‚"""

        if context and context.get("collected_info"):
            info = context["collected_info"]
            base_prompt += f"""

å½“å‰å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
- å…»æ®–åœº: {info.get('farm_name', 'å¾…æ”¶é›†')}
- å‘ç—…æ—¥æœŸ: {info.get('onset_date', 'å¾…æ”¶é›†')}
- ç¦½ç±»: {info.get('poultry_type', 'å¾…æ”¶é›†')} {info.get('breed', '')}
- æ‚£ç—…æ•°é‡: {info.get('affected_count', 'å¾…æ”¶é›†')}
- ç—‡çŠ¶: {', '.join(info.get('symptoms', [])) or 'å¾…æ”¶é›†'}

è¿˜éœ€è¦æ”¶é›†: {', '.join(context.get('pending_questions', []))}
"""

        return base_prompt
```

### æ¨¡å‹åˆ‡æ¢åœºæ™¯

#### åœºæ™¯1ï¼šMasteråœ¨åå°åˆ‡æ¢é»˜è®¤æ¨¡å‹
```python
# Masteræ“ä½œï¼šå°†é»˜è®¤æ¨¡å‹ä»é€šä¹‰åƒé—®åˆ‡æ¢åˆ°GPT-4
@app.patch("/api/v1/admin/ai-models/{model_id}/set-default")
async def set_default_model(model_id: UUID, current_user: User):
    if current_user.role != "master":
        raise HTTPException(403, "åªæœ‰Masterå¯ä»¥åˆ‡æ¢æ¨¡å‹")

    # å–æ¶ˆå½“å‰é»˜è®¤
    await db.execute(
        "UPDATE ai_models SET is_default = FALSE WHERE is_default = TRUE"
    )

    # è®¾ç½®æ–°é»˜è®¤
    await db.execute(
        "UPDATE ai_models SET is_default = TRUE WHERE id = :id",
        {"id": model_id}
    )

    # è®°å½•åˆ‡æ¢æ—¥å¿—
    await log_model_switch(model_id, current_user.id)

    return {"message": "é»˜è®¤æ¨¡å‹å·²åˆ‡æ¢", "model_id": model_id}
```

#### åœºæ™¯2ï¼šç”¨æˆ·ç»§ç»­å¯¹è¯ï¼ˆæ— æ„Ÿåˆ‡æ¢ï¼‰
```
ç”¨æˆ·è§†è§’ï¼š
1. ç»§ç»­å’ŒAIå¯¹è¯
2. AIå›å¤é£æ ¼å¯èƒ½ç•¥æœ‰å˜åŒ–ï¼ˆä¸åŒæ¨¡å‹ç‰¹ç‚¹ï¼‰
3. ä½†ä¹‹å‰è¯´è¿‡çš„ä¿¡æ¯AIéƒ½è®°å¾—
4. ç—…å†ä¿¡æ¯ç»§ç»­ç´¯ç§¯

æŠ€æœ¯å®ç°ï¼š
1. ç”¨æˆ·å‘æ¶ˆæ¯
2. åç«¯è·å–å½“å‰é»˜è®¤æ¨¡å‹ï¼ˆå·²è¢«Masteråˆ‡æ¢ï¼‰
3. åŠ è½½å®Œæ•´å¯¹è¯å†å²
4. ç”¨æ–°æ¨¡å‹ç”Ÿæˆå›å¤
5. ä¿å­˜å›å¤ï¼ˆæ ‡è®°ä½¿ç”¨çš„æ¨¡å‹ï¼‰
```

### å‰ç«¯æ˜¾ç¤º

#### å¯¹è¯ç•Œé¢æ ‡è®°ï¼ˆå¯é€‰ï¼‰
```vue
<template>
  <view class="message" :class="msg.role">
    <view class="content">{{ msg.content }}</view>

    <!-- å¯é€‰ï¼šæ˜¾ç¤ºAIå›å¤ä½¿ç”¨çš„æ¨¡å‹ -->
    <view v-if="msg.role === 'assistant' && showModelTag" class="model-tag">
      {{ formatModelName(msg.model_used) }}
    </view>
  </view>
</template>

<script>
export default {
  methods: {
    formatModelName(model) {
      const names = {
        'qwen/qwen-plus': 'é€šä¹‰åƒé—®',
        'openai/gpt-4o': 'GPT-4',
        'claude/claude-3-sonnet': 'Claude'
      }
      return names[model] || model
    }
  }
}
</script>

<style>
.model-tag {
  font-size: 10px;
  color: #999;
  margin-top: 4px;
}
</style>
```

#### å·²æ”¶é›†ä¿¡æ¯é¢æ¿
```vue
<template>
  <view class="collected-info-panel">
    <text class="title">å·²æ”¶é›†ä¿¡æ¯</text>

    <view class="info-item" v-for="(value, key) in collectedInfo">
      <text class="label">{{ labelMap[key] }}</text>
      <text class="value">{{ value || 'å¾…æ”¶é›†' }}</text>
      <view class="confidence" :style="{ width: confidence[key] * 100 + '%' }"/>
    </view>

    <view class="pending" v-if="pendingQuestions.length">
      <text>è¿˜éœ€ç¡®è®¤: {{ pendingQuestions.join('ã€') }}</text>
    </view>
  </view>
</template>
```

### æ•°æ®ä¸€è‡´æ€§ä¿è¯

#### 1. ä¸Šä¸‹æ–‡åŸå­æ›´æ–°
```python
async def update_context(conversation_id: UUID, new_info: dict):
    """åŸå­æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡"""
    async with db.transaction():
        # è·å–å½“å‰ä¸Šä¸‹æ–‡
        conversation = await db.fetch_one(
            "SELECT context FROM conversations WHERE id = :id FOR UPDATE",
            {"id": conversation_id}
        )

        # åˆå¹¶æ–°ä¿¡æ¯
        context = conversation.context or {}
        collected = context.get("collected_info", {})

        for key, value in new_info.items():
            if value and (key not in collected or new_info.get(f"{key}_confidence", 0) >
                         context.get("confidence", {}).get(key, 0)):
                collected[key] = value

        context["collected_info"] = collected

        # ä¿å­˜
        await db.execute(
            "UPDATE conversations SET context = :context WHERE id = :id",
            {"context": json.dumps(context), "id": conversation_id}
        )
```

#### 2. æ¶ˆæ¯é¡ºåºä¿è¯
```python
# ä½¿ç”¨æ—¶é—´æˆ³ç¡®ä¿æ¶ˆæ¯é¡ºåº
await self.save_message(
    conversation_id=conversation_id,
    role="assistant",
    content=response["content"],
    timestamp=datetime.utcnow()  # ç²¾ç¡®æ—¶é—´æˆ³
)
```

## æ•°æ®åº“è®¾è®¡

### ai_models è¡¨ï¼ˆAIæ¨¡å‹é…ç½®è¡¨ï¼‰
```sql
CREATE TABLE ai_models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    provider VARCHAR(50) NOT NULL,  -- qwen, minimax, openai, claude, deepseek
    model_name VARCHAR(100) NOT NULL,  -- qwen-max, abab6-chat, gpt-4o
    display_name VARCHAR(100) NOT NULL,  -- æ˜¾ç¤ºåç§°
    api_endpoint TEXT NOT NULL,
    api_key_encrypted TEXT NOT NULL,  -- åŠ å¯†å­˜å‚¨
    is_active BOOLEAN DEFAULT TRUE,
    is_default BOOLEAN DEFAULT FALSE,  -- é»˜è®¤æ¨¡å‹
    config JSONB,  -- æ¨¡å‹ç‰¹å®šé…ç½®
    usage_limit JSONB,  -- ä½¿ç”¨é™åˆ¶
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL,
    created_by UUID REFERENCES users(id)
);

CREATE INDEX idx_models_provider ON ai_models(provider);
CREATE INDEX idx_models_active ON ai_models(is_active);
CREATE INDEX idx_models_default ON ai_models(is_default) WHERE is_default = TRUE;
```

### ai_model_config å­—æ®µç¤ºä¾‹
```json
{
  "temperature": 0.7,
  "max_tokens": 2000,
  "top_p": 0.9,
  "frequency_penalty": 0,
  "presence_penalty": 0,
  "timeout_seconds": 30,
  "retry_attempts": 3,
  "supports_function_calling": true,
  "supports_streaming": true
}
```

### ai_usage_logs è¡¨ï¼ˆAIä½¿ç”¨æ—¥å¿—ï¼‰
```sql
CREATE TABLE ai_usage_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    model_id UUID REFERENCES ai_models(id),
    user_id UUID REFERENCES users(id),
    conversation_id UUID REFERENCES conversations(id),
    request_tokens INTEGER,
    response_tokens INTEGER,
    total_tokens INTEGER,
    cost DECIMAL(10,4),  -- æˆæœ¬ï¼ˆå…ƒï¼‰
    latency_ms INTEGER,  -- å“åº”å»¶è¿Ÿ
    status VARCHAR(20),  -- success, error, timeout
    error_message TEXT,
    created_at TIMESTAMP NOT NULL
);

CREATE INDEX idx_usage_model ON ai_usage_logs(model_id);
CREATE INDEX idx_usage_user ON ai_usage_logs(user_id);
CREATE INDEX idx_usage_date ON ai_usage_logs(created_at);
```

## ç»Ÿä¸€é€‚é…å™¨è®¾è®¡

### LLMé€‚é…å™¨æ¥å£
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, AsyncIterator

class BaseLLMAdapter(ABC):
    """AIæ¨¡å‹é€‚é…å™¨åŸºç±»"""

    def __init__(self, api_key: str, api_endpoint: str, config: Dict):
        self.api_key = api_key
        self.api_endpoint = api_endpoint
        self.config = config

    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """èŠå¤©è¡¥å…¨"""
        pass

    @abstractmethod
    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼èŠå¤©è¡¥å…¨"""
        pass

    @abstractmethod
    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pass
```

### é€šä¹‰åƒé—®é€‚é…å™¨
```python
import dashscope
from dashscope import Generation

class QwenAdapter(BaseLLMAdapter):
    """é€šä¹‰åƒé—®é€‚é…å™¨"""

    PRICING = {
        'qwen-max': {'input': 0.04, 'output': 0.12},  # å…ƒ/åƒtokens
        'qwen-plus': {'input': 0.008, 'output': 0.024},
        'qwen-turbo': {'input': 0.002, 'output': 0.006}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, '', config)
        self.model_name = model_name
        dashscope.api_key = api_key

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            response = Generation.call(
                model=self.model_name,
                messages=messages,
                tools=self._convert_functions(functions) if functions else None,
                result_format='message',
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000)
            )

            if response.status_code == 200:
                return {
                    'content': response.output.choices[0].message.content,
                    'function_call': self._parse_function_call(response),
                    'usage': {
                        'input_tokens': response.usage.input_tokens,
                        'output_tokens': response.usage.output_tokens,
                        'total_tokens': response.usage.total_tokens
                    }
                }
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")

        except Exception as e:
            raise Exception(f"é€šä¹‰åƒé—®è°ƒç”¨é”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        responses = Generation.call(
            model=self.model_name,
            messages=messages,
            result_format='message',
            stream=True,
            incremental_output=True,
            temperature=self.config.get('temperature', 0.7)
        )

        for response in responses:
            if response.status_code == 200:
                yield response.output.choices[0].message.content

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['qwen-plus'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return input_cost + output_cost

    def _convert_functions(self, functions: List[Dict]) -> List[Dict]:
        """è½¬æ¢å‡½æ•°å®šä¹‰ä¸ºé€šä¹‰æ ¼å¼"""
        return [
            {
                'type': 'function',
                'function': func
            }
            for func in functions
        ]

    def _parse_function_call(self, response) -> Optional[Dict]:
        """è§£æå‡½æ•°è°ƒç”¨"""
        choice = response.output.choices[0]
        if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
            tool_call = choice.message.tool_calls[0]
            return {
                'name': tool_call.function.name,
                'arguments': tool_call.function.arguments
            }
        return None
```

### MiniMaxé€‚é…å™¨
```python
import httpx

class MiniMaxAdapter(BaseLLMAdapter):
    """MiniMaxé€‚é…å™¨"""

    PRICING = {
        'abab6-chat': {'input': 0.015, 'output': 0.015},  # å…ƒ/åƒtokens
        'abab5.5-chat': {'input': 0.005, 'output': 0.005}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, 'https://api.minimax.chat/v1/text/chatcompletion_v2', config)
        self.model_name = model_name
        self.group_id = config.get('group_id')

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨MiniMax API"""
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        payload = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.config.get('temperature', 0.7),
            'top_p': self.config.get('top_p', 0.9),
            'max_tokens': self.config.get('max_tokens', 2000)
        }

        if functions:
            payload['functions'] = functions

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.api_endpoint}?GroupId={self.group_id}",
                headers=headers,
                json=payload,
                timeout=30.0
            )

            if response.status_code == 200:
                data = response.json()
                return {
                    'content': data['choices'][0]['message']['content'],
                    'function_call': data['choices'][0]['message'].get('function_call'),
                    'usage': {
                        'input_tokens': data['usage']['prompt_tokens'],
                        'output_tokens': data['usage']['completion_tokens'],
                        'total_tokens': data['usage']['total_tokens']
                    }
                }
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.text}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        # MiniMaxæµå¼å®ç°
        # ...

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['abab6-chat'])
        total_tokens = input_tokens + output_tokens
        return (total_tokens / 1000) * pricing['input']
```

### OpenAIé€‚é…å™¨ â­
```python
from openai import AsyncOpenAI

class OpenAIAdapter(BaseLLMAdapter):
    """OpenAI GPTé€‚é…å™¨"""

    PRICING = {
        'gpt-4o': {'input': 0.005, 'output': 0.015},  # $/åƒtokens
        'gpt-4-turbo': {'input': 0.01, 'output': 0.03},
        'gpt-4': {'input': 0.03, 'output': 0.06},
        'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, config.get('base_url', 'https://api.openai.com/v1'), config)
        self.model_name = model_name
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=self.api_endpoint,
            timeout=config.get('timeout_seconds', 30)
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨OpenAI API"""
        try:
            kwargs = {
                'model': self.model_name,
                'messages': messages,
                'temperature': self.config.get('temperature', 0.7),
                'max_tokens': self.config.get('max_tokens', 2000),
                'top_p': self.config.get('top_p', 0.9)
            }

            # æ·»åŠ Function Calling
            if functions:
                kwargs['tools'] = [
                    {'type': 'function', 'function': func}
                    for func in functions
                ]
                kwargs['tool_choice'] = 'auto'

            response = await self.client.chat.completions.create(**kwargs)

            result = {
                'content': response.choices[0].message.content,
                'usage': {
                    'input_tokens': response.usage.prompt_tokens,
                    'output_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                }
            }

            # è§£æFunction Call
            if response.choices[0].message.tool_calls:
                tool_call = response.choices[0].message.tool_calls[0]
                result['function_call'] = {
                    'name': tool_call.function.name,
                    'arguments': tool_call.function.arguments
                }

            return result

        except Exception as e:
            raise Exception(f"OpenAIè°ƒç”¨é”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        stream = await self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=self.config.get('temperature', 0.7),
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒè½¬äººæ°‘å¸ï¼ŒæŒ‰7.2æ±‡ç‡ï¼‰"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['gpt-4o'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return (input_cost + output_cost) * 7.2  # è½¬æ¢ä¸ºäººæ°‘å¸
```

### Claudeé€‚é…å™¨ â­
```python
from anthropic import AsyncAnthropic

class ClaudeAdapter(BaseLLMAdapter):
    """Anthropic Claudeé€‚é…å™¨"""

    PRICING = {
        'claude-3-opus-20240229': {'input': 0.015, 'output': 0.075},  # $/åƒtokens
        'claude-3-sonnet-20240229': {'input': 0.003, 'output': 0.015},
        'claude-3-haiku-20240307': {'input': 0.00025, 'output': 0.00125}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, 'https://api.anthropic.com', config)
        self.model_name = model_name
        self.client = AsyncAnthropic(
            api_key=api_key,
            timeout=config.get('timeout_seconds', 30)
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨Claude API"""
        try:
            # åˆ†ç¦»systemæ¶ˆæ¯ï¼ˆClaudeè¦æ±‚systemå•ç‹¬ä¼ ï¼‰
            system_message = ""
            chat_messages = []

            for msg in messages:
                if msg['role'] == 'system':
                    system_message = msg['content']
                else:
                    chat_messages.append(msg)

            kwargs = {
                'model': self.model_name,
                'messages': chat_messages,
                'max_tokens': self.config.get('max_tokens', 2000),
                'temperature': self.config.get('temperature', 0.7)
            }

            if system_message:
                kwargs['system'] = system_message

            # æ·»åŠ Tool Useï¼ˆClaudeçš„Function Callingï¼‰
            if functions:
                kwargs['tools'] = [
                    {
                        'name': func['name'],
                        'description': func.get('description', ''),
                        'input_schema': func.get('parameters', {})
                    }
                    for func in functions
                ]

            response = await self.client.messages.create(**kwargs)

            # è§£æå“åº”
            content = ""
            function_call = None

            for block in response.content:
                if block.type == 'text':
                    content = block.text
                elif block.type == 'tool_use':
                    function_call = {
                        'name': block.name,
                        'arguments': json.dumps(block.input)
                    }

            return {
                'content': content,
                'function_call': function_call,
                'usage': {
                    'input_tokens': response.usage.input_tokens,
                    'output_tokens': response.usage.output_tokens,
                    'total_tokens': response.usage.input_tokens + response.usage.output_tokens
                }
            }

        except Exception as e:
            raise Exception(f"Claudeè°ƒç”¨é”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        # åˆ†ç¦»systemæ¶ˆæ¯
        system_message = ""
        chat_messages = []

        for msg in messages:
            if msg['role'] == 'system':
                system_message = msg['content']
            else:
                chat_messages.append(msg)

        kwargs = {
            'model': self.model_name,
            'messages': chat_messages,
            'max_tokens': self.config.get('max_tokens', 2000),
            'stream': True
        }

        if system_message:
            kwargs['system'] = system_message

        async with self.client.messages.stream(**kwargs) as stream:
            async for text in stream.text_stream:
                yield text

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒè½¬äººæ°‘å¸ï¼ŒæŒ‰7.2æ±‡ç‡ï¼‰"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['claude-3-sonnet-20240229'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return (input_cost + output_cost) * 7.2  # è½¬æ¢ä¸ºäººæ°‘å¸
```

### Kimié€‚é…å™¨ (æœˆä¹‹æš—é¢) â­
```python
class KimiAdapter(BaseLLMAdapter):
    """Kimi/Moonshoté€‚é…å™¨ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰"""

    PRICING = {
        'moonshot-v1-8k': {'input': 0.012, 'output': 0.012},    # å…ƒ/åƒtokens
        'moonshot-v1-32k': {'input': 0.024, 'output': 0.024},
        'moonshot-v1-128k': {'input': 0.06, 'output': 0.06}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, 'https://api.moonshot.cn/v1', config)
        self.model_name = model_name
        # Kimiå…¼å®¹OpenAI SDK
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=self.api_endpoint,
            timeout=config.get('timeout_seconds', 60)  # Kimié•¿ä¸Šä¸‹æ–‡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨Kimi API"""
        try:
            kwargs = {
                'model': self.model_name,
                'messages': messages,
                'temperature': self.config.get('temperature', 0.7),
                'max_tokens': self.config.get('max_tokens', 4096)
            }

            # Kimiæ”¯æŒFunction Calling
            if functions:
                kwargs['tools'] = [
                    {'type': 'function', 'function': func}
                    for func in functions
                ]

            response = await self.client.chat.completions.create(**kwargs)

            result = {
                'content': response.choices[0].message.content,
                'usage': {
                    'input_tokens': response.usage.prompt_tokens,
                    'output_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                }
            }

            if response.choices[0].message.tool_calls:
                tool_call = response.choices[0].message.tool_calls[0]
                result['function_call'] = {
                    'name': tool_call.function.name,
                    'arguments': tool_call.function.arguments
                }

            return result

        except Exception as e:
            raise Exception(f"Kimiè°ƒç”¨é”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        stream = await self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=self.config.get('temperature', 0.7),
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['moonshot-v1-32k'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return input_cost + output_cost
```

### è…¾è®¯æ··å…ƒé€‚é…å™¨ â­
```python
import hashlib
import hmac
import json
from datetime import datetime

class HunyuanAdapter(BaseLLMAdapter):
    """è…¾è®¯æ··å…ƒé€‚é…å™¨"""

    PRICING = {
        'hunyuan-lite': {'input': 0.008, 'output': 0.008},    # å…ƒ/åƒtokens
        'hunyuan-standard': {'input': 0.045, 'output': 0.045},
        'hunyuan-pro': {'input': 0.1, 'output': 0.1}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        # è…¾è®¯äº‘ä½¿ç”¨SecretIdå’ŒSecretKey
        self.secret_id = config.get('secret_id')
        self.secret_key = api_key  # è¿™é‡Œapi_keyå­˜å‚¨secret_key
        self.region = config.get('region', 'ap-guangzhou')
        self.model_name = model_name
        self.endpoint = 'hunyuan.tencentcloudapi.com'
        self.config = config

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨è…¾è®¯æ··å…ƒAPI"""
        try:
            # æ„å»ºè¯·æ±‚
            action = 'ChatCompletions'
            payload = {
                'Model': self.model_name,
                'Messages': [
                    {'Role': msg['role'], 'Content': msg['content']}
                    for msg in messages
                ],
                'Temperature': self.config.get('temperature', 0.7),
                'TopP': self.config.get('top_p', 0.9)
            }

            # ç­¾åå’Œè¯·æ±‚
            headers = self._build_headers(action, payload)

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"https://{self.endpoint}",
                    headers=headers,
                    json=payload,
                    timeout=30.0
                )

                data = response.json()

                if 'Response' in data and 'Choices' in data['Response']:
                    choice = data['Response']['Choices'][0]
                    usage = data['Response'].get('Usage', {})

                    return {
                        'content': choice['Message']['Content'],
                        'usage': {
                            'input_tokens': usage.get('PromptTokens', 0),
                            'output_tokens': usage.get('CompletionTokens', 0),
                            'total_tokens': usage.get('TotalTokens', 0)
                        }
                    }
                else:
                    raise Exception(f"APIè¿”å›é”™è¯¯: {data}")

        except Exception as e:
            raise Exception(f"è…¾è®¯æ··å…ƒè°ƒç”¨é”™è¯¯: {str(e)}")

    def _build_headers(self, action: str, payload: dict) -> dict:
        """æ„å»ºè…¾è®¯äº‘APIç­¾åå¤´"""
        timestamp = int(datetime.now().timestamp())
        date = datetime.utcnow().strftime('%Y-%m-%d')

        # ç®€åŒ–çš„ç­¾åé€»è¾‘ï¼Œå®é™…ä½¿ç”¨éœ€è¦å®Œæ•´çš„TC3-HMAC-SHA256ç­¾å
        # å»ºè®®ä½¿ç”¨è…¾è®¯äº‘SDK: tencentcloud-sdk-python
        return {
            'Content-Type': 'application/json',
            'X-TC-Action': action,
            'X-TC-Version': '2023-09-01',
            'X-TC-Timestamp': str(timestamp),
            'X-TC-Region': self.region,
            'Authorization': self._sign_request(action, payload, timestamp, date)
        }

    def _sign_request(self, action: str, payload: dict, timestamp: int, date: str) -> str:
        """TC3-HMAC-SHA256ç­¾åï¼ˆç®€åŒ–ç‰ˆï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨SDKï¼‰"""
        # å®é™…å®ç°è¯·å‚è€ƒè…¾è®¯äº‘æ–‡æ¡£
        # https://cloud.tencent.com/document/api/1729/105701
        pass

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        # è…¾è®¯æ··å…ƒæ”¯æŒSSEæµå¼
        # å®ç°ç±»ä¼¼ï¼Œè®¾ç½®Stream=True
        pass

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['hunyuan-standard'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return input_cost + output_cost
```

**æ¨èï¼šä½¿ç”¨è…¾è®¯äº‘å®˜æ–¹SDK**
```python
# pip install tencentcloud-sdk-python
from tencentcloud.hunyuan.v20230901 import hunyuan_client, models

class HunyuanSDKAdapter(BaseLLMAdapter):
    """è…¾è®¯æ··å…ƒé€‚é…å™¨ï¼ˆä½¿ç”¨å®˜æ–¹SDKï¼‰"""

    def __init__(self, api_key: str, model_name: str, config: Dict):
        from tencentcloud.common import credential
        from tencentcloud.common.profile.client_profile import ClientProfile

        cred = credential.Credential(
            config.get('secret_id'),
            api_key  # secret_key
        )
        client_profile = ClientProfile()
        self.client = hunyuan_client.HunyuanClient(cred, config.get('region', 'ap-guangzhou'), client_profile)
        self.model_name = model_name
        self.config = config

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨æ··å…ƒAPI"""
        req = models.ChatCompletionsRequest()
        req.Model = self.model_name
        req.Messages = [
            models.Message(Role=msg['role'], Content=msg['content'])
            for msg in messages
        ]

        resp = self.client.ChatCompletions(req)

        return {
            'content': resp.Choices[0].Message.Content,
            'usage': {
                'input_tokens': resp.Usage.PromptTokens,
                'output_tokens': resp.Usage.CompletionTokens,
                'total_tokens': resp.Usage.TotalTokens
            }
        }
```

### Geminié€‚é…å™¨ (Google)
```python
import google.generativeai as genai

class GeminiAdapter(BaseLLMAdapter):
    """Google Geminié€‚é…å™¨"""

    PRICING = {
        'gemini-1.5-pro': {'input': 0.00125, 'output': 0.005},    # $/åƒtokens
        'gemini-1.5-flash': {'input': 0.000075, 'output': 0.0003},
        'gemini-1.0-pro': {'input': 0.0005, 'output': 0.0015}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, '', config)
        self.model_name = model_name
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨Gemini API"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            gemini_messages = self._convert_messages(messages)

            # é…ç½®ç”Ÿæˆå‚æ•°
            generation_config = genai.GenerationConfig(
                temperature=self.config.get('temperature', 0.7),
                max_output_tokens=self.config.get('max_tokens', 4096),
                top_p=self.config.get('top_p', 0.9)
            )

            # åˆ›å»ºå¯¹è¯
            chat = self.model.start_chat(history=gemini_messages[:-1])

            # å‘é€æœ€åä¸€æ¡æ¶ˆæ¯
            response = await chat.send_message_async(
                gemini_messages[-1]['parts'][0],
                generation_config=generation_config
            )

            # ä¼°ç®—tokenï¼ˆGeminiä¸ç›´æ¥è¿”å›tokenæ•°ï¼Œéœ€è¦è®¡ç®—ï¼‰
            input_text = ' '.join([m.get('content', '') for m in messages])
            output_text = response.text

            return {
                'content': response.text,
                'usage': {
                    'input_tokens': self._estimate_tokens(input_text),
                    'output_tokens': self._estimate_tokens(output_text),
                    'total_tokens': self._estimate_tokens(input_text + output_text)
                }
            }

        except Exception as e:
            raise Exception(f"Geminiè°ƒç”¨é”™è¯¯: {str(e)}")

    def _convert_messages(self, messages: List[Dict[str, str]]) -> List[Dict]:
        """è½¬æ¢ä¸ºGeminiæ ¼å¼"""
        gemini_messages = []

        for msg in messages:
            if msg['role'] == 'system':
                # Geminiæ²¡æœ‰systemè§’è‰²ï¼Œä½œä¸ºç”¨æˆ·ç¬¬ä¸€æ¡æ¶ˆæ¯
                gemini_messages.insert(0, {
                    'role': 'user',
                    'parts': [f"[System Instructions]: {msg['content']}"]
                })
            elif msg['role'] == 'user':
                gemini_messages.append({
                    'role': 'user',
                    'parts': [msg['content']]
                })
            elif msg['role'] == 'assistant':
                gemini_messages.append({
                    'role': 'model',
                    'parts': [msg['content']]
                })

        return gemini_messages

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°ï¼ˆç®€å•æŒ‰å­—ç¬¦æ•°/4ä¼°ç®—ï¼‰"""
        return len(text) // 4

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        gemini_messages = self._convert_messages(messages)
        chat = self.model.start_chat(history=gemini_messages[:-1])

        response = await chat.send_message_async(
            gemini_messages[-1]['parts'][0],
            stream=True
        )

        async for chunk in response:
            if chunk.text:
                yield chunk.text

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒè½¬äººæ°‘å¸ï¼‰"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['gemini-1.5-pro'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return (input_cost + output_cost) * 7.2
```

### DeepSeeké€‚é…å™¨
```python
class DeepSeekAdapter(BaseLLMAdapter):
    """DeepSeeké€‚é…å™¨ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰"""

    PRICING = {
        'deepseek-chat': {'input': 0.001, 'output': 0.002},  # å…ƒ/åƒtokens
        'deepseek-coder': {'input': 0.001, 'output': 0.002}
    }

    def __init__(self, api_key: str, model_name: str, config: Dict):
        super().__init__(api_key, config.get('base_url', 'https://api.deepseek.com/v1'), config)
        self.model_name = model_name
        # DeepSeekå…¼å®¹OpenAI SDK
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=self.api_endpoint,
            timeout=config.get('timeout_seconds', 30)
        )

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Dict:
        """è°ƒç”¨DeepSeek APIï¼ˆä¸OpenAIæ ¼å¼ç›¸åŒï¼‰"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                max_tokens=self.config.get('max_tokens', 2000)
            )

            return {
                'content': response.choices[0].message.content,
                'usage': {
                    'input_tokens': response.usage.prompt_tokens,
                    'output_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                }
            }

        except Exception as e:
            raise Exception(f"DeepSeekè°ƒç”¨é”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[str]:
        """æµå¼å“åº”"""
        stream = await self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model_name, self.PRICING['deepseek-chat'])
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        return input_cost + output_cost
```

### é€‚é…å™¨å·¥å‚
```python
class LLMAdapterFactory:
    """LLMé€‚é…å™¨å·¥å‚"""

    ADAPTERS = {
        'qwen': QwenAdapter,
        'minimax': MiniMaxAdapter,
        'kimi': KimiAdapter,
        'hunyuan': HunyuanAdapter,
        'openai': OpenAIAdapter,
        'claude': ClaudeAdapter,
        'gemini': GeminiAdapter,
        'deepseek': DeepSeekAdapter
    }

    @staticmethod
    def create_adapter(
        provider: str,
        api_key: str,
        model_name: str,
        config: Dict
    ) -> BaseLLMAdapter:
        """åˆ›å»ºé€‚é…å™¨"""
        adapter_class = LLMAdapterFactory.ADAPTERS.get(provider)
        if not adapter_class:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")

        return adapter_class(api_key, model_name, config)

    @staticmethod
    async def get_default_adapter() -> BaseLLMAdapter:
        """è·å–é»˜è®¤é€‚é…å™¨"""
        model = await db.query(AIModel).filter(
            AIModel.is_active == True,
            AIModel.is_default == True
        ).first()

        if not model:
            raise Exception("æœªé…ç½®é»˜è®¤AIæ¨¡å‹")

        # è§£å¯†APIå¯†é’¥
        api_key = decrypt_api_key(model.api_key_encrypted)

        return LLMAdapterFactory.create_adapter(
            model.provider,
            api_key,
            model.model_name,
            model.config
        )
```

## AIæ¨¡å‹ç®¡ç†API

### é…ç½®æ¨¡å‹ï¼ˆä»…Masterï¼‰
```
POST /api/v1/admin/ai-models
Authorization: Bearer {master_token}
```

**Request**
```json
{
  "provider": "qwen",
  "model_name": "qwen-max",
  "display_name": "é€šä¹‰åƒé—®-æœ€å¼ºç‰ˆ",
  "api_key": "sk-xxxxxxxxxxxxx",
  "api_endpoint": "",
  "is_default": true,
  "config": {
    "temperature": 0.7,
    "max_tokens": 2000,
    "top_p": 0.9,
    "timeout_seconds": 30
  }
}
```

### è·å–æ¨¡å‹åˆ—è¡¨
```
GET /api/v1/admin/ai-models
```

**Response**
```json
{
  "models": [
    {
      "id": "uuid",
      "provider": "qwen",
      "model_name": "qwen-max",
      "display_name": "é€šä¹‰åƒé—®-æœ€å¼ºç‰ˆ",
      "is_active": true,
      "is_default": true,
      "config": {...},
      "created_at": "2026-01-27T00:00:00Z"
    },
    {
      "id": "uuid",
      "provider": "minimax",
      "model_name": "abab6-chat",
      "display_name": "MiniMax-æ ‡å‡†ç‰ˆ",
      "is_active": true,
      "is_default": false,
      "config": {...},
      "created_at": "2026-01-27T00:00:00Z"
    }
  ]
}
```

### è®¾ç½®é»˜è®¤æ¨¡å‹
```
PATCH /api/v1/admin/ai-models/{model_id}/set-default
```

### æµ‹è¯•æ¨¡å‹
```
POST /api/v1/admin/ai-models/{model_id}/test
```

**Request**
```json
{
  "test_message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
}
```

**Response**
```json
{
  "success": true,
  "response": "ä½ å¥½ï¼æˆ‘æ˜¯...",
  "latency_ms": 1200,
  "tokens": {
    "input": 10,
    "output": 50,
    "total": 60
  },
  "cost": 0.003
}
```

### æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡
```
GET /api/v1/admin/ai-models/usage-stats
```

**Query Parameters**:
- start_date, end_date
- model_id
- user_id

**Response**
```json
{
  "total_requests": 1523,
  "total_tokens": 523400,
  "total_cost": 45.67,
  "by_model": [
    {
      "model_name": "qwen-max",
      "requests": 1200,
      "tokens": 450000,
      "cost": 38.5,
      "avg_latency_ms": 1500
    },
    {
      "model_name": "abab6-chat",
      "requests": 323,
      "tokens": 73400,
      "cost": 7.17,
      "avg_latency_ms": 800
    }
  ],
  "by_user": [
    {
      "user_name": "å¼ åŒ»ç”Ÿ",
      "requests": 567,
      "tokens": 198000,
      "cost": 16.8
    }
  ]
}
```

## å‰ç«¯é…ç½®ç•Œé¢

### Masterç®¡ç†åå° - AIæ¨¡å‹é…ç½®
```vue
<template>
  <view class="ai-model-config">
    <view class="header">
      <text class="title">AIæ¨¡å‹é…ç½®</text>
      <button @click="showAddModel">æ·»åŠ æ¨¡å‹</button>
    </view>

    <view class="model-list">
      <view v-for="model in models" :key="model.id" class="model-card">
        <view class="model-info">
          <text class="name">{{ model.display_name }}</text>
          <text class="provider">{{ model.provider }} - {{ model.model_name }}</text>
          <view class="badges">
            <view v-if="model.is_default" class="badge default">é»˜è®¤</view>
            <view v-if="model.is_active" class="badge active">å¯ç”¨</view>
            <view v-else class="badge inactive">åœç”¨</view>
          </view>
        </view>

        <view class="model-stats">
          <text>ä½¿ç”¨æ¬¡æ•°: {{ model.usage_count }}</text>
          <text>æ€»æˆæœ¬: Â¥{{ model.total_cost }}</text>
        </view>

        <view class="actions">
          <button @click="testModel(model)">æµ‹è¯•</button>
          <button @click="setDefault(model)" :disabled="model.is_default">
            è®¾ä¸ºé»˜è®¤
          </button>
          <button @click="toggleActive(model)">
            {{ model.is_active ? 'åœç”¨' : 'å¯ç”¨' }}
          </button>
          <button @click="editModel(model)">ç¼–è¾‘</button>
        </view>
      </view>
    </view>

    <!-- æ·»åŠ /ç¼–è¾‘æ¨¡å‹å¼¹çª— -->
    <uni-popup ref="modelPopup" type="center">
      <view class="model-form">
        <text class="form-title">é…ç½®AIæ¨¡å‹</text>

        <picker @change="selectProvider" :range="providers" range-key="name">
          <view class="field">
            <text>AIæä¾›å•†</text>
            <text>{{ selectedProvider.name }}</text>
          </view>
        </picker>

        <picker @change="selectModel" :range="availableModels" range-key="name">
          <view class="field">
            <text>æ¨¡å‹</text>
            <text>{{ selectedModel.name }}</text>
          </view>
        </picker>

        <input v-model="formData.display_name" placeholder="æ˜¾ç¤ºåç§°" />
        <input v-model="formData.api_key" type="password" placeholder="API Key" />

        <view class="config-section">
          <text>é«˜çº§é…ç½®</text>
          <slider v-model="formData.temperature" min="0" max="1" step="0.1" />
          <text>Temperature: {{ formData.temperature }}</text>

          <input v-model.number="formData.max_tokens" type="number" placeholder="æœ€å¤§Tokens" />
        </view>

        <checkbox v-model="formData.is_default">è®¾ä¸ºé»˜è®¤æ¨¡å‹</checkbox>

        <view class="form-actions">
          <button @click="saveModel">ä¿å­˜</button>
          <button @click="closePopup">å–æ¶ˆ</button>
        </view>
      </view>
    </uni-popup>

    <!-- ä½¿ç”¨ç»Ÿè®¡ -->
    <view class="usage-stats">
      <text class="section-title">ä½¿ç”¨ç»Ÿè®¡</text>
      <view class="chart">
        <!-- ä½¿ç”¨echartså±•ç¤ºç»Ÿè®¡å›¾è¡¨ -->
      </view>
    </view>
  </view>
</template>

<script>
export default {
  data() {
    return {
      models: [],
      providers: [
        { value: 'qwen', name: 'é€šä¹‰åƒé—®ï¼ˆé˜¿é‡Œäº‘ï¼‰', icon: 'ğŸ‡¨ğŸ‡³' },
        { value: 'minimax', name: 'MiniMax', icon: 'ğŸ‡¨ğŸ‡³' },
        { value: 'openai', name: 'OpenAI', icon: 'ğŸ‡ºğŸ‡¸' },
        { value: 'claude', name: 'Claude', icon: 'ğŸ‡ºğŸ‡¸' },
        { value: 'deepseek', name: 'DeepSeek', icon: 'ğŸ‡¨ğŸ‡³' }
      ],
      modelOptions: {
        'qwen': [
          { value: 'qwen-max', name: 'Qwen Max (æœ€å¼º)', desc: 'å¤æ‚ä»»åŠ¡' },
          { value: 'qwen-plus', name: 'Qwen Plus (å¹³è¡¡)', desc: 'æ—¥å¸¸ä½¿ç”¨' },
          { value: 'qwen-turbo', name: 'Qwen Turbo (å¿«é€Ÿ)', desc: 'ç®€å•ä»»åŠ¡' }
        ],
        'minimax': [
          { value: 'abab6-chat', name: 'Abab 6 (æ ‡å‡†)', desc: 'ç»¼åˆèƒ½åŠ›å¼º' },
          { value: 'abab5.5-chat', name: 'Abab 5.5 (ç»æµ)', desc: 'æ€§ä»·æ¯”é«˜' }
        ]
      }
    }
  },
  methods: {
    async testModel(model) {
      uni.showLoading({ title: 'æµ‹è¯•ä¸­...' })
      try {
        const res = await this.$api.testAIModel(model.id, {
          test_message: 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±'
        })
        uni.showModal({
          title: 'æµ‹è¯•æˆåŠŸ',
          content: `å“åº”: ${res.response}\nå»¶è¿Ÿ: ${res.latency_ms}ms\næˆæœ¬: Â¥${res.cost}`,
          showCancel: false
        })
      } catch (e) {
        uni.showToast({ title: 'æµ‹è¯•å¤±è´¥', icon: 'none' })
      } finally {
        uni.hideLoading()
      }
    }
  }
}
</script>
```

## ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹

### .env
```bash
# é»˜è®¤AIæ¨¡å‹é…ç½®ï¼ˆå¯é€šè¿‡ç®¡ç†åå°è¦†ç›–ï¼‰

# é€šä¹‰åƒé—®
QWEN_API_KEY=sk-xxxxxxxxxx
QWEN_MODEL=qwen-plus

# MiniMax
MINIMAX_API_KEY=xxxxx
MINIMAX_GROUP_ID=xxxxx
MINIMAX_MODEL=abab6-chat

# OpenAI (å¤‡ç”¨)
OPENAI_API_KEY=sk-xxxxxxxxxx
OPENAI_MODEL=gpt-4o
OPENAI_BASE_URL=https://api.openai.com/v1

# Claude (å¤‡ç”¨)
CLAUDE_API_KEY=sk-xxxxxxxxxx
CLAUDE_MODEL=claude-3-sonnet-20240229
```

## æˆæœ¬æ§åˆ¶

### ä½¿ç”¨é™åˆ¶
```python
# åœ¨ai_modelsè¡¨çš„usage_limitå­—æ®µ
{
  "daily_requests": 1000,      # æ¯æ—¥è¯·æ±‚æ•°é™åˆ¶
  "daily_tokens": 1000000,     # æ¯æ—¥tokené™åˆ¶
  "daily_cost": 100.0,         # æ¯æ—¥æˆæœ¬é™åˆ¶ï¼ˆå…ƒï¼‰
  "per_user_daily_requests": 50  # å•ç”¨æˆ·æ¯æ—¥é™åˆ¶
}
```

### é™åˆ¶æ£€æŸ¥
```python
async def check_usage_limit(model_id: UUID, user_id: UUID) -> bool:
    """æ£€æŸ¥æ˜¯å¦è¶…è¿‡ä½¿ç”¨é™åˆ¶"""
    model = await get_model(model_id)
    limits = model.usage_limit

    # æ£€æŸ¥æ¨¡å‹æ¯æ—¥é™åˆ¶
    today_usage = await get_today_usage(model_id)
    if limits.get('daily_requests') and today_usage['requests'] >= limits['daily_requests']:
        raise Exception("æ¨¡å‹æ¯æ—¥è¯·æ±‚æ•°å·²è¾¾ä¸Šé™")

    # æ£€æŸ¥ç”¨æˆ·æ¯æ—¥é™åˆ¶
    user_today_usage = await get_user_today_usage(user_id, model_id)
    if limits.get('per_user_daily_requests') and \
       user_today_usage['requests'] >= limits['per_user_daily_requests']:
        raise Exception("æ‚¨ä»Šæ—¥çš„AIä½¿ç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™")

    return True
```

## åˆå§‹åŒ–è„šæœ¬

### é»˜è®¤æ¨¡å‹é…ç½®
```python
async def init_default_models():
    """åˆå§‹åŒ–é»˜è®¤AIæ¨¡å‹é…ç½®"""

    # é€šä¹‰åƒé—®ï¼ˆé»˜è®¤ï¼‰
    qwen_model = AIModel(
        provider='qwen',
        model_name='qwen-plus',
        display_name='é€šä¹‰åƒé—®-æ ‡å‡†ç‰ˆ',
        api_endpoint='',
        api_key_encrypted=encrypt_api_key(os.getenv('QWEN_API_KEY')),
        is_active=True,
        is_default=True,
        config={
            'temperature': 0.7,
            'max_tokens': 2000,
            'top_p': 0.9,
            'timeout_seconds': 30
        }
    )

    # MiniMax
    minimax_model = AIModel(
        provider='minimax',
        model_name='abab6-chat',
        display_name='MiniMax-æ ‡å‡†ç‰ˆ',
        api_endpoint='https://api.minimax.chat/v1/text/chatcompletion_v2',
        api_key_encrypted=encrypt_api_key(os.getenv('MINIMAX_API_KEY')),
        is_active=True,
        is_default=False,
        config={
            'temperature': 0.7,
            'max_tokens': 2000,
            'group_id': os.getenv('MINIMAX_GROUP_ID')
        }
    )

    db.add_all([qwen_model, minimax_model])
    await db.commit()
```

## ç›‘æ§å’Œå‘Šè­¦

### ç›‘æ§æŒ‡æ ‡
- APIè°ƒç”¨æˆåŠŸç‡
- å¹³å‡å“åº”å»¶è¿Ÿ
- Tokenæ¶ˆè€—é€Ÿç‡
- æˆæœ¬è¶‹åŠ¿
- é”™è¯¯ç±»å‹åˆ†å¸ƒ

### å‘Šè­¦è§„åˆ™
```python
ALERT_RULES = {
    'high_error_rate': {
        'threshold': 0.1,  # 10%é”™è¯¯ç‡
        'action': 'switch_to_backup_model'
    },
    'high_latency': {
        'threshold': 5000,  # 5ç§’
        'action': 'notify_admin'
    },
    'daily_cost_exceeded': {
        'threshold': 100,  # 100å…ƒ/å¤©
        'action': 'pause_non_critical_usage'
    }
}
```
